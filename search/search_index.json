{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Fairway Data Ingestion","text":"<p>fairway is a portable, scalable data ingestion framework designed for sustainable management of centralized research data. </p>"},{"location":"#core-philosophy","title":"Core Philosophy","text":"<p>Traditional data ingestion often suffers from undocumented transformations, rigid pipelines, and difficult-to-scale infrastructure. fairway addresses these pain points by being:</p> <ul> <li>Config-Driven: Define your pipeline in YAML, not just code.</li> <li>Engine-Agnostic: Shift from local DuckDB processing to distributed PySpark on Slurm with a single config change.</li> <li>Orchestration-Native: Built to run seamlessly with Nextflow and Slurm.</li> <li>Validation-First: Multi-level sanity and distribution checks are baked into the pipeline.</li> </ul>"},{"location":"#where-to-start","title":"Where to Start?","text":"<ul> <li>Getting Started: Install fairway and run your first pipeline.</li> <li>Architecture: Understand the underlying pipeline lifecycle and tech stack.</li> <li>Configuration Guide: Learn how to define sources, metadata, and validations.</li> <li>Custom Transformations: Extend the pipeline with your own processing logic.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>fairway is designed as a modular pipeline that abstracts away the complexities of distributed computing and data orchestration.</p>"},{"location":"architecture/#high-level-workflow","title":"High-Level Workflow","text":"<p>The pipeline operates in three distinct phases:</p>"},{"location":"architecture/#phase-i-discovery-triggering","title":"Phase I: Discovery &amp; Triggering","text":"<ol> <li>File Discovery: fairway scans the landing zone defined in the config.</li> <li>Manifest Check: It compares incoming files against the <code>fmanifest</code> to skip files that have already been processed with the same hash.</li> </ol>"},{"location":"architecture/#phase-ii-raw-processing","title":"Phase II: Raw Processing","text":"<p>The goal of this phase is to convert vendor data into a queryable, verifiable format without altering the original schema.</p> <ol> <li>Validation (Level 1): Basic sanity checks (e.g., column counts).</li> <li>Ingestion: Conversion of source formats (CSV, etc.) to Parquet, optimized with partitioning and \"salting\" to prevent data skew in Spark.</li> <li>Enrichment: Optional geospatial enrichment (geocoding, H3 indexing).</li> <li>Validation (Level 2): Content-level checks (e.g., null spikes, schema adherence).</li> <li>Summarization: Automatic generation of summary statistics and markdown reports.</li> </ol>"},{"location":"architecture/#phase-iii-transformation","title":"Phase III: Transformation","text":"<p>This phase reshapes the data for specific analytical needs.</p> <ol> <li>Custom Logic: Execution of project-specific Python transformations.</li> <li>Lineage: Manifest updates that link transformed products back to their raw sources.</li> </ol>"},{"location":"architecture/#tech-stack","title":"Tech Stack","text":""},{"location":"architecture/#orchestration","title":"Orchestration","text":"<ul> <li>Nextflow: Manages the dependency graph and executes tasks in parallel.</li> <li>Slurm: Handles resource allocation and job scheduling on HPC clusters.</li> </ul>"},{"location":"architecture/#compute-engines","title":"Compute Engines","text":"<ul> <li>DuckDB: Used for local, single-node processing. Exceptionally fast for smaller-than-memory datasets.</li> <li>PySpark: Leveraged for large-scale distributed processing on Slurm clusters.</li> </ul>"},{"location":"architecture/#storage","title":"Storage","text":"<ul> <li>Parquet: The primary storage format. It provides efficient columnar storage and is natively supported by both DuckDB and Spark.</li> <li>fmanifest: A JSON-based manifest system that tracks file hashes, processing status, and metadata.</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>fairway pipelines are driven by YAML configuration files. This allows you to define data sources, metadata extraction, validations, and enrichments without writing pipeline code.</p>"},{"location":"configuration/#root-options","title":"Root Options","text":"Field Description Default <code>dataset_name</code> A unique identifier for the dataset. Required <code>engine</code> Data processing engine (<code>duckdb</code> or <code>pyspark</code>). <code>duckdb</code> <code>partition_by</code> List of columns to partition the output Parquet files by. <code>[]</code>"},{"location":"configuration/#data-sources","title":"Data Sources","text":"<p>The <code>sources</code> section defines where your raw data lives and how to identify it.</p> <pre><code>sources:\n  - name: \"provider_extract\"\n    path_pattern: \"data/raw/provider_*.csv\"\n    naming_pattern: \"provider_(?P&lt;state&gt;[A-Z]{2})_(?P&lt;date&gt;\\\\d{8})\\\\.csv\"\n    format: \"csv\"\n</code></pre>"},{"location":"configuration/#source-expansion","title":"Source Expansion","text":"<p>fairway uses <code>glob</code> to discover files matching the <code>path_pattern</code>. Each discovered file becomes a separate task in the pipeline.</p>"},{"location":"configuration/#metadata-extraction","title":"Metadata Extraction","text":"<p>If a <code>naming_pattern</code> (Python regex) is provided, fairway extracts named groups from the filename and injects them as columns into the data. In the example above, a file named <code>provider_CT_20230101.csv</code> will have <code>state='CT'</code> and <code>date='20230101'</code> added to every row.</p>"},{"location":"configuration/#validations","title":"Validations","text":"<p>fairway supports multi-level validations to ensure data quality.</p> <pre><code>validations:\n  level1:\n    min_rows: 100\n  level2:\n    check_nulls:\n      - \"provider_id\"\n      - \"state\"\n</code></pre> <ul> <li>Level 1: Basic sanity checks (e.g., minimum row counts).</li> <li>Level 2: Schema and distribution checks (e.g., checking for nulls in mandatory columns).</li> </ul>"},{"location":"configuration/#enrichment","title":"Enrichment","text":"<p>Enable built-in enrichments like geospatial processing:</p> <pre><code>enrichment:\n  geocode: true\n</code></pre>"},{"location":"configuration/#custom-transformations","title":"Custom Transformations","text":"<p>If your data requires complex reshaping, you can point to a custom transformation script:</p> <pre><code>data:\n  transformation: \"my_custom_transform\"\n</code></pre> <p>The pipeline will look for a class in <code>src/transformations/my_custom_transform.py</code> that implements the transformation logic.</p>"},{"location":"data_inventory/","title":"Data Product Inventory","text":"<p>This page provides an inventory of all data products managed by the fairway framework, including their current status and summary statistics.</p>"},{"location":"data_inventory/#available-datasets","title":"Available Datasets","text":"<p>Stay tuned! This section will be automatically populated with data product summaries as they are generated by the pipeline.</p>"},{"location":"data_inventory/#example-data-product-discovery_test","title":"Example Data Product: <code>discovery_test</code>","text":"<ul> <li>Status: <code>Success</code></li> <li>Last Run: 2026-01-08</li> <li>Row Count: 500,000</li> <li>Reports: View Report # Placeholder for actual report path</li> </ul>"},{"location":"engines/","title":"Compute Engines","text":"<p>fairway supports multiple compute engines, allowing you to choose the best tool for your data size and environment.</p>"},{"location":"engines/#duckdb-engine","title":"DuckDB Engine","text":"<p>The DuckDB engine is the default for local development and smaller datasets.</p> <ul> <li>When to use: Testing, local development, datasets that fit on a single machine.</li> <li>Key Features:<ul> <li>Zero-dependency (embedded database).</li> <li>Highly optimized for analytical queries on Parquet files.</li> <li>Supports SQL-based data manipulation.</li> </ul> </li> </ul>"},{"location":"engines/#pyspark-engine","title":"PySpark Engine","text":"<p>The PySpark engine is designed for large-scale distributed processing.</p> <ul> <li>When to use: Large datasets (TB+), high-compute tasks, running on Slurm/YCRC clusters.</li> <li>Key Features:<ul> <li>Distributed Processing: Scales across multiple nodes.</li> <li>Resource Management: Integrates with Slurm to dynamically provision clusters.</li> <li>Data Skew Protection: Implements \"salting\" techniques to ensure balanced partitioning.</li> <li>Metadata Injection: Efficiently adds external metadata (like state or date from filenames) to billions of rows.</li> </ul> </li> </ul>"},{"location":"engines/#configuring-engines","title":"Configuring Engines","text":"<p>You can switch engines in your YAML configuration:</p> <pre><code>engine: \"pyspark\" # options: \"duckdb\", \"pyspark\"\n</code></pre> <p>When using <code>pyspark</code> on a cluster, ensure you run with the <code>--profile slurm --with-spark</code> flags to manage the Spark cluster lifecycle.</p>"},{"location":"enrichments/","title":"Geospatial Enrichments","text":"<p>fairway includes built-in support for geospatial data enrichment, making it easy to turn raw address or coordinate data into analysis-ready spatial indices.</p>"},{"location":"enrichments/#geocoding","title":"Geocoding","text":"<p>When enabled, fairway can resolve address fields to latitude and longitude coordinates.</p> <ul> <li>Enable in Config:     <code>yaml     enrichment:       geocode: true</code></li> </ul>"},{"location":"enrichments/#h3-indexing","title":"H3 Indexing","text":"<p>For efficient spatial joining and aggregation, fairway can assign H3 indices (Hexagonal Hierarchical Spatial Index) to your data.</p> <ul> <li>Why H3?: Hexagons minimize quantization error when aggregating spatial data and have uniform distances between neighbors.</li> </ul>"},{"location":"enrichments/#geo-digest","title":"Geo-Digest","text":"<p>The pipeline automatically generates a \"geo-digest\" \u2013 a summary of the spatial distribution of your data, identifying potential outliers or data density issues.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you set up fairway and run your first data ingestion pipeline.</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Nextflow</li> <li>Apptainer (for containerized execution)</li> </ul>"},{"location":"getting-started/#setup","title":"Setup","text":"<p>Clone the repository and install the package in editable mode:</p> <pre><code>git clone https://github.com/DISSC/fairway.git\ncd fairway\npip install -e .\n</code></pre>"},{"location":"getting-started/#initializing-a-project","title":"Initializing a Project","text":"<p>To start a new ingestion project, use the <code>init</code> command:</p> <pre><code>fairway init my-data-project\ncd my-data-project\n</code></pre> <p>This creates the recommended directory structure: *   <code>config/</code>: Store your YAML configuration files. *   <code>data/</code>: Local storage for raw, intermediate, and final data. *   <code>src/transformations/</code>: Custom Python scripts for data reshaping. *   <code>docs/</code>: Project-specific documentation. *   <code>logs/</code>: Slurm and Nextflow execution logs.</p>"},{"location":"getting-started/#running-a-pipeline","title":"Running a Pipeline","text":""},{"location":"getting-started/#local-execution-duckdb","title":"Local Execution (DuckDB)","text":"<p>For small datasets or testing, run fairway locally using the default <code>standard</code> profile which uses DuckDB:</p> <pre><code>fairway run --config config/discovery_test.yaml\n</code></pre>"},{"location":"getting-started/#slurm-execution-pyspark","title":"Slurm Execution (PySpark)","text":"<p>To run on a Slurm cluster with PySpark support:</p> <pre><code>fairway run --config config/discovery_test.yaml --profile slurm --with-spark\n</code></pre> <p>This command: 1.  Provisions a Spark-on-Slurm cluster. 2.  Submits the Nextflow pipeline to the Slurm controller. 3.  Executes the ingestion using the PySpark engine.</p>"},{"location":"transformations/","title":"Custom Transformations","text":"<p>While fairway handles the heavy lifting of ingestion and validation, datasets often require unique reshaping logic (e.g., long-to-wide transformations or complex aggregations).</p>"},{"location":"transformations/#writing-a-transformer","title":"Writing a Transformer","text":"<p>To create a custom transformer: 1.  Create a new Python file in <code>src/transformations/</code>. 2.  Define a class that takes a DataFrame (Pandas or Spark, depending on the engine) and implements a <code>transform()</code> method.</p>"},{"location":"transformations/#example-my_transformpy","title":"Example: <code>my_transform.py</code>","text":"<pre><code>class MyTransformer:\n    def __init__(self, df):\n        self.df = df\n\n    def transform(self):\n        # Your custom logic here\n        self.df['new_column'] = self.df['existing_column'] * 2\n        return self.df\n</code></pre>"},{"location":"transformations/#registering-the-transformer","title":"Registering the Transformer","text":"<p>Link your transformer to a dataset in the YAML config:</p> <pre><code>data:\n  transformation: \"my_transform\"\n</code></pre> <p>fairway will dynamically load <code>MyTransformer</code> from <code>src/transformations/my_transform.py</code> and apply it to the data during Phase III of the pipeline.</p>"},{"location":"transformations/#transformation-lineage","title":"Transformation Lineage","text":"<p>fairway tracks the lineage of transformed data products, ensuring you can always trace a final table back to the specific raw source file and transformation script version used to create it.</p>"},{"location":"validations/","title":"Validations","text":"<p>Data quality is a core pillar of fairway. The framework provides built-in validation levels that can be configured per dataset.</p>"},{"location":"validations/#level-1-sanity-checks","title":"Level 1: Sanity Checks","text":"<p>Level 1 checks are basic sanity checks that ensure the data \"looks\" right at a high level.</p> <ul> <li>Config Options:<ul> <li><code>min_rows</code>: Fails the pipeline if the row count is below this threshold.</li> </ul> </li> <li>Example:     <code>yaml     validations:       level1:         min_rows: 1000</code></li> </ul>"},{"location":"validations/#level-2-schema-distribution-checks","title":"Level 2: Schema &amp; Distribution Checks","text":"<p>Level 2 checks look deeper into the content of the data.</p> <ul> <li>Config Options:<ul> <li><code>check_nulls</code>: A list of columns that must not contain any null values.</li> </ul> </li> <li>Example:     <code>yaml     validations:       level2:         check_nulls:           - \"user_id\"           - \"transaction_date\"</code></li> </ul>"},{"location":"validations/#validation-reports","title":"Validation Reports","text":"<p>When a pipeline runs, fairway generates a markdown report for each source file in <code>data/final/</code>. This report includes:</p> <ul> <li>Status of all validation checks.</li> <li>Summary statistics (row counts, etc.).</li> <li>Error messages for failed checks.</li> </ul> <p>If any validation fails, the file is marked as <code>failed</code> in the manifest and will need to be addressed before subsequent phases.</p>"},{"location":"design_docs/iceberg_support/","title":"Implementation Plan - Iceberg Support","text":"<p>This document outlines the changes required to support Apache Iceberg as an output format for <code>fairway</code>.</p>"},{"location":"design_docs/iceberg_support/#complexity-assessment","title":"Complexity Assessment","text":"<p>Level: Medium</p> <p>Supporting Iceberg introduces \"Table\" semantics into a pipeline that is currently heavily \"File-centric\". *   Dependencies: Requires adding <code>pyiceberg</code> for local catalog and table management. *   Pipeline Logic: The current <code>overwrite</code> logic (deleting directories) must be replaced with <code>INSERT OVERWRITE</code> or <code>REPLACE TABLE</code> semantics provided by Iceberg. *   Downstream: <code>RedivisExporter</code> currently expects files. It will need to be adapted to export a snapshot of the Iceberg table to Parquet before uploading.</p>"},{"location":"design_docs/iceberg_support/#proposed-changes","title":"Proposed Changes","text":""},{"location":"design_docs/iceberg_support/#1-dependencies","title":"1. Dependencies","text":"<ul> <li>Add <code>pyiceberg</code> to <code>requirements.txt</code> / <code>pyproject.toml</code>.</li> <li>Ensure <code>duckdb</code> version is compatible (current 1.4.3 is sufficient, but requires <code>install iceberg</code>).</li> </ul>"},{"location":"design_docs/iceberg_support/#2-configuration","title":"2. Configuration","text":"<p>Update <code>config.yaml</code> to support <code>iceberg</code> specific settings.</p> <pre><code>storage:\n  type: \"iceberg\" # vs \"files\" (default)\n  catalog:\n    type: \"sql\" # or \"glue\", \"rest\", \"hive\"\n    uri: \"sqlite:///path/to/catalog.db\" # Example for local\n    warehouse: \"data/iceberg_warehouse\"\n</code></pre>"},{"location":"design_docs/iceberg_support/#3-engine-refactor","title":"3. Engine Refactor","text":"<ul> <li>Modify <code>DuckDBEngine</code> or create <code>IcebergEngine</code>.</li> <li>Write Path: Instead of <code>COPY ... TO ...</code>, use <code>pyiceberg</code> to create the table schema if not exists, then use DuckDB to write (via Arrow or experimental Iceberg writes if configured with an external catalog).<ul> <li>Alternative: Use <code>pyiceberg</code> to write data directly (slower than DuckDB) OR use DuckDB's <code>experimental_httpfs</code> + Iceberg extension to write to the catalog if possible.</li> <li>Recommended: Use <code>pyiceberg</code> to manage metadata/create table. Convert DuckDB result to Arrow <code>df.to_arrow_table()</code> and use <code>pyiceberg_table.append(arrow_table)</code>.</li> </ul> </li> </ul>"},{"location":"design_docs/iceberg_support/#4-pipeline-updates","title":"4. Pipeline Updates","text":"<ul> <li><code>src/pipeline.py</code> needs to branch logic based on output format.<ul> <li>If <code>Parquet</code>: Keep existing <code>ingest_csv</code> -&gt; <code>query</code> -&gt; <code>to_parquet</code> flow.</li> <li>If <code>Iceberg</code>: <code>ingest_csv</code> (to temp) -&gt; <code>query</code> -&gt; <code>write_to_iceberg</code> (append/replace).</li> </ul> </li> </ul>"},{"location":"design_docs/iceberg_support/#5-redivis-export","title":"5. Redivis Export","text":"<ul> <li>Iceberg tables are directories of files + metadata. Redivis expects a file upload.</li> <li>Update <code>RedivisExporter</code> to accept a DuckDB connection and a table name.</li> <li>Exporter will execute <code>COPY (SELECT * FROM iceberg_table) TO 'temp.parquet' (FORMAT PARQUET)</code> and then upload <code>temp.parquet</code>.</li> </ul>"},{"location":"design_docs/iceberg_support/#verification-plan","title":"Verification Plan","text":""},{"location":"design_docs/iceberg_support/#automated-tests","title":"Automated Tests","text":"<ol> <li>Create <code>tests/test_iceberg_ingestion.py</code>.</li> <li>Test full flow: CSV -&gt; Iceberg Table (verify via <code>pyiceberg</code> reading) -&gt; Redivis Export (mocked).</li> </ol>"},{"location":"design_docs/iceberg_support/#manual-verification","title":"Manual Verification","text":"<ol> <li>Run <code>fairway</code> with <code>format: iceberg</code>.</li> <li>Inspect <code>data/final</code> (or warehouse dir) to see <code>metadata/</code> and <code>data/</code> folders structure.</li> <li>Use <code>duckdb</code> CLI to query the generated Iceberg table.</li> </ol>"}]}