dataset_name: "{name}"
engine: "{engine}" # 'duckdb' or 'pyspark'

storage:
  raw_dir: "data/raw"
  intermediate_dir: "data/intermediate"
  final_dir: "data/final"
  format: "parquet" # Options: parquet, csv, json

# Optional: Partition intermediate and final data
# partition_by: 
#   - year
#   - month

sources:
  - name: "example_source"
    path: "data/raw/example.csv"
    format: "csv" # Options: csv, parquet, json
    # You can specify schema inline or point to a file (generated by fairway generate-schema)
    schema:
      id: "BIGINT"
      value: "DOUBLE"
    # schema: "config/example_source_schema.yaml"

    # Optional: Metadata for the source
    metadata:
      description: "An example dataset"
      owner: "Research Team"

validations:
  level1:
    min_rows: 1
    # check_column_count: true
  level2: {{}}
    # SQL-based checks
    # - "id IS NOT NULL"

enrichment:
  geocode: false
  # census_tracts: false

# Redivis Export Configuration (Optional)
# redivis:
#   user_name: "your_username"
#   dataset_name: "{name}"
#   upload_merge_strategy: "replace" # append, replace, fail

# Custom Transformation Script (Optional)
# data:
#   transformation: "src/transformations/example_transform.py"
