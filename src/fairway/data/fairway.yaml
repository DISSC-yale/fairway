# Fairway Configuration Template
# This file configures the data ingestion pipeline, including sources, schema, validation, and enrichment.

# Name of the dataset being processed
dataset_name: "{name}"

# Execution Engine: 'duckdb' (local/single-node) or 'pyspark' (distributed)
# Options: 'duckdb', 'pyspark'
# Default is 'duckdb' if not specified.
engine: "{engine_type}" 

# Storage Configuration
storage:
  # Directory for raw input files
  raw_dir: "data/raw"
  
  # Directory for intermediate processed files (e.g., converted to Parquet)
  intermediate_dir: "data/intermediate"
  
  # Directory for final output files
  final_dir: "data/final"

# Data Sources Configuration
sources:
  - name: "sales"
    # Path to the input data. Supports glob patterns or directories.
    # Pointing to the root of a partitioned dataset lets the engine handle discovery.
    # New in Manifest V2: defining 'root' allows for relocatable manifests.
    root: "data/raw" 
    path: "sales_partitioned" # Relative to root if specified
    hive_partitioning: true
    
    # regex pattern to extract metadata from filenames
    # naming_pattern: "sales_(?P<year>\\d{{4}})_(?P<month>\\d{{2}}).csv"
    
    # Input format: 'csv', 'json', 'parquet'
    # Options: 'csv', 'json', 'parquet'
    format: "csv"
    
    # Schema definition (Optional but recommended for strict validation)
    # Maps column names to their expected types.
    # Note: You can generate this schema using `fairway schema-generation`
    schema:
      id: "BIGINT"
      date: "TIMESTAMP"
      amount: "DOUBLE"
      address: "STRING"
      year: "INTEGER"
      month: "INTEGER"

    # Engine-specific read options (passed directly to read_csv/spark.read.options)
    # read_options:
    #   header: false
    #   skip: 1 
    #   delim: "|"
    
    # Preprocessing Configuration
    # preprocess:
    #   action: "unzip" # or path to script: "scripts/my_script.py"
    #   scope: "per_file" # or 'global'
    #   execution_mode: "driver" # or 'cluster' (requires pyspark)

  
    partition_by:
      - "year"
      - "month"

    # Static metadata to attach to all records from this source
    # metadata:
    #   source_system: "legacy_sales_db"
    #   environment: "production"



# Validation Rules
validations:
  # Level 1: Basic Sanity Checks
  level1:
    min_rows: 1
    # max_rows: 1000000
    
  # Level 2: Schema and Data Quality Checks
  level2:
    # check_nulls:
    #   - "id"
    #   - "date"
    #   - "amount"
    # check_unique:
    #   - "id"

# Data Enrichment Configuration
enrichment:
  # Enable geocoding for addresses (requires 'address' column)
  geocode: false
  
  # Add H3 spatial index (requires lat/lon or geocoding)
  # h3_index: false
  # h3_resolution: 9

# Performance & Optimization
performance:
  # Target number of rows per output file (for re-partitioning/salting)
  target_rows: 500000

# Redivis Export Configuration (Optional)
# redivis:
#   user: "username"
#   dataset: "dataset_name"
#   upload_merge_strategy: "append" # or 'replace', 'fail'
