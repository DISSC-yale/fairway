# Fairway Configuration Template
# This file configures the data ingestion pipeline, including sources, schema, validation, and enrichment.

# Name of the dataset being processed
dataset_name: "{name}"

# Execution Engine: 'duckdb' (local/single-node) or 'pyspark' (distributed)
# Default is 'duckdb' if not specified.
engine: "{engine_type}" 

# Storage Configuration
storage:
  # Directory for raw input files
  raw_dir: "data/raw"
  
  # Directory for intermediate processed files (e.g., converted to Parquet)
  intermediate_dir: "data/intermediate"
  
  # Directory for final output files
  final_dir: "data/final"

# Data Sources Configuration
sources:
  - name: "sales"
    # Path to the input data. Supports glob patterns or directories.
    # Pointing to the root of a partitioned dataset lets the engine handle discovery.
    path: "data/raw/sales_partitioned"
    
    # regex pattern to extract metadata from filenames
    # naming_pattern: "sales_(?P<year>\\d{4})_(?P<month>\\d{2}).csv"
    
    # Input format: 'csv', 'json', 'parquet'
    format: "csv"
    
    # Schema definition (Optional but recommended for strict validation)
    # Maps column names to their expected types.
    # Note: You can generate this schema using `fairway schema-generation`
    schema:
      id: "BIGINT"
      date: "TIMESTAMP"
      amount: "DOUBLE"
      address: "STRING"
      year: "INTEGER"
      month: "INTEGER"

    # Static metadata to attach to all records from this source
    # metadata:
    #   source_system: "legacy_sales_db"
    #   environment: "production"

# Partitioning Strategy
# Columns to partition the output by.
partition_by:
  - "year"
  - "month"

# Validation Rules
validations:
  # Level 1: Basic Sanity Checks
  level1:
    min_rows: 1
    # max_rows: 1000000
    
  # Level 2: Schema and Data Quality Checks
  level2:
    # check_nulls:
    #   - "id"
    #   - "date"
    #   - "amount"
    # check_unique:
    #   - "id"

# Data Enrichment Configuration
enrichment:
  # Enable geocoding for addresses (requires 'address' column)
  geocode: false
  
  # Add H3 spatial index (requires lat/lon or geocoding)
  # h3_index: false
  # h3_resolution: 9

# Performance & Optimization
performance:
  # Target number of rows per output file (for re-partitioning/salting)
  target_rows: 500000

# Redivis Export Configuration (Optional)
# redivis:
#   user: "username"
#   dataset: "dataset_name"
#   upload_merge_strategy: "append" # or 'replace', 'fail'

# Custom Transformation Script (Optional)
# Path to a python script with a custom transformer class
# data:
#   transformation: "src/fairway/transformations/custom_cleaner.py"
