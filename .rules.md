# Project Rules

## Long Term Rules

- **CLI Build Process**: CLI should have a build process for building the container (implemented as `fairway build`).
- **Job Management**: CLI should include `status` and `kill` commands to manage Slurm jobs (wrapping `squeue` and `scancel`).
- **Architecture**: Decoupled Worker. `fairway run` MUST NOT launch Nextflow. It is a worker process.

## Scalability
- **Data Collection**: Methods like `toPandas()` or `collect()` MUST NOT be used on full datasets in production paths. Use iterators, sampling, or native engine operations.
- **Engine Agnostic**: Where possible, downstream components (Validators, Enrichers) should be engine-agnostic or support multiple backends (Pandas, Spark, DuckDB) to avoid bottlenecks.

## Apptainer Templates
- **Variable Definitions**: `Apptainer.def` templates MUST define necessary build variables (e.g., `SPARK_VERSION`, `HADOOP_VERSION`) within the `%post` section before they are used.
- **Single Source of Truth**: All templates MUST be stored as static files in `src/fairway/data/` and loaded via `_read_data_file`. `src/fairway/templates.py` should act as a loader.

## Version Consistency
- **Spark Version**: Ensure the Spark version is consistent across all configuration files (Use Spark 4.1.1):
    - `pyproject.toml`
    - `src/fairway/data/Apptainer.def`
    - `src/fairway/data/fairway-hpc.sh` (or generated scripts)

## Execution Standards
- **Zero-Dependency Bootstrap**: Execution wrappers (e.g., `Makefile`) MUST robustly handle missing dependencies.
    - Nextflow Logic: Check PATH -> Check `./nextflow` -> Check `module avail` -> Download from web.
- **Driver Jobs**: The preferred method for HPC submission is the "Driver Job" pattern:
    - Submit the orchestrator (Nextflow) as a small single-task Slurm job (`scripts/driver.sh`).
    - This job then runs Nextflow, which submits the actual worker tasks.

## Generated Structure
- **Scripts Directory**: Helper scripts (e.g., `driver.sh`, `fairway-hpc.sh`) MUST be placed in `scripts/` during project initialization.

## Testing
- **Local Testing**: Run all tests using `pytest` from the project root.
    ```bash
    pytest tests/
    ```
- **Import Isolation**: Ensure the pipeline remains importable even if optional dependencies (like DuckDB) are missing.
    - Run the isolation test: `python tests/test_imports_isolation.py`
    - (Note: This is also covered by `pytest` but useful for verification in clean environments).
- **Environment**: Some tests (e.g., PySpark) require a compatible Java environment. If testing locally on a machine without Spark/Java properly configured, these tests may fail or be skipped.
- **Integration Testing**: When testing project initialization:
    - Create a temporary directory (e.g., `test_proj`).
    - Run `fairway init` and verify the output.
    - **CRITICAL**: Delete the temporary directory immediately after verification to prevent clutter. 
