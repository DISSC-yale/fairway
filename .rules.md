# Project Rules

## Long Term Rules

- **CLI Build Process**: CLI should have a build process for building the container (implemented as `fairway build`).
- **Job Management**: CLI should include `status` and `kill` commands to manage Slurm jobs (wrapping `squeue` and `scancel`).
- **Architecture**: Decoupled Worker. `fairway run` MUST NOT launch Nextflow. It is a worker process.

## Apptainer Templates
- **Variable Definitions**: `Apptainer.def` templates MUST define necessary build variables (e.g., `SPARK_VERSION`, `HADOOP_VERSION`) within the `%post` section before they are used.
- **Single Source of Truth**: All templates MUST be stored as static files in `src/fairway/data/` and loaded via `_read_data_file`. `src/fairway/templates.py` should act as a loader.

## Version Consistency
- **Spark Version**: Ensure the Spark version is consistent across all configuration files (Use Spark 4.1.1):
    - `pyproject.toml`
    - `src/fairway/data/Apptainer.def`
    - `src/fairway/data/fairway-hpc.sh` (or generated scripts)

## Testing
- **Local Testing**: Run all tests using `pytest` from the project root.
    ```bash
    pytest tests/
    ```
- **Import Isolation**: Ensure the pipeline remains importable even if optional dependencies (like DuckDB) are missing.
    - Run the isolation test: `python tests/test_imports_isolation.py`
    - (Note: This is also covered by `pytest` but useful for verification in clean environments).
- **Environment**: Some tests (e.g., PySpark) require a compatible Java environment. If testing locally on a machine without Spark/Java properly configured, these tests may fail or be skipped.
